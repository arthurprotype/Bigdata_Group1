{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 生物大数据分析课程设计源代码\n",
    "\n",
    "项目名称：通过深度学习对癫痫脑电波信号进行分类\n",
    "\n",
    "项目成员：叶佳晨、吴玥汀、石京成、谢乐凡、韩德坤"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 导入所需的库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential, Model, Input, load_model\n",
    "from keras.layers import Dense, Dropout, LSTM, Bidirectional, GRU, Conv1D, BatchNormalization, Input, concatenate, SpatialDropout1D, GlobalAveragePooling1D, MaxPooling1D, Softmax, Activation\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras import optimizers\n",
    "from keras import regularizers\n",
    "from keras.layers.core import Masking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对标签进行独热编码\n",
    "label_dic = {\n",
    "    \"normal\": (1, 0, 0), # 健康人\n",
    "    \"inter\": (0, 1, 0), # 患病但未发作\n",
    "    \"ictal\": (0, 0, 1) # 患病且发作\n",
    "}\n",
    "# 文件夹路径\n",
    "path = \"./EEG_data/\"\n",
    "# 滑动窗口大小\n",
    "window_size = 256\n",
    "# 滑动步长\n",
    "step_size = 128\n",
    "# 测试集比例\n",
    "test_size = 0.2\n",
    "\n",
    "\n",
    "# 读取目标文件夹中的所有文件\n",
    "def folder_to_df(letter):\n",
    "    full_path = path + letter + \"/*.*\"\n",
    "    files = glob (full_path)\n",
    "    df_list = []\n",
    "    for file in files:\n",
    "        df_list.append (pd.read_csv (file, header = None))\n",
    "    big_df = pd.concat (df_list, ignore_index=True, axis= 1)\n",
    "\n",
    "    return big_df.T\n",
    "\n",
    "# 窗口切割，窗口大小为256，步长为128\n",
    "def window (a, w = 256, o = 128):\n",
    "    view = []\n",
    "    for i in range (0, a.shape[0] - w, o):\n",
    "        sub = a[i:i+w]\n",
    "        \"\"\"\n",
    "        MinMax = np.ptp (sub)\n",
    "        sub = (sub - np.mean (sub)) / np.std (sub)\n",
    "        sub = list (sub)\n",
    "        sub.append (MinMax)\n",
    "        \"\"\"\n",
    "        view += [sub]\n",
    "    \n",
    "    return view.copy()\n",
    "\n",
    "# 数据扩充\n",
    "def enrich_data (df): \n",
    "    res = []\n",
    "    for i in range (df.shape[0]):\n",
    "        res += [window (df.iloc[i].values, w = window_size, o = step_size)]\n",
    "    return res\n",
    "\n",
    "# 读取所有的文件夹\n",
    "def load_data_as_df():\n",
    "    A = folder_to_df('Z')\n",
    "    B = folder_to_df('O')\n",
    "    C = folder_to_df('N')\n",
    "    D = folder_to_df('F')\n",
    "    E = folder_to_df('S')\n",
    "    \n",
    "    normal = A.append (B).reset_index(drop = True)\n",
    "    interictal = C.append (D).reset_index(drop = True)\n",
    "    ictal = E\n",
    "\n",
    "    return normal, interictal, ictal\n",
    "\n",
    "# 数据扩充\n",
    "def format_enrich_data(normal, interictal, ictal):\n",
    "    \n",
    "    # 对扩充后的数据集进行维度转换\n",
    "    normal_data_enr = np.asarray (enrich_data(normal)).reshape(-1, np.asarray(enrich_data(normal)).shape[-1])\n",
    "    interictal_data_enr = np.asarray (enrich_data(interictal)).reshape(-1, np.asarray(enrich_data(interictal)).shape[-1])\n",
    "    ictal_data_enr = np.asarray (enrich_data(ictal)).reshape(-1, np.asarray(enrich_data(ictal)).shape[-1])\n",
    "\n",
    "    # 更换数据形式\n",
    "    normal_data_enr_df = pd.DataFrame(normal_data_enr)\n",
    "    interictal_data_enr_df = pd.DataFrame(interictal_data_enr)\n",
    "    ictal_data_enr_df = pd.DataFrame(ictal_data_enr)\n",
    "    \n",
    "    # 打标签\n",
    "    normal_data_enr_lab = pd.DataFrame ([label_dic['normal'] for _ in range (normal_data_enr.shape[0])])\n",
    "    interictal_data_enr_lab = pd.DataFrame ([label_dic['inter'] for _ in range (interictal_data_enr.shape[0])])\n",
    "    ictal_data_enr_lab = pd.DataFrame ([label_dic['ictal'] for _ in range (ictal_data_enr.shape[0])])\n",
    "\n",
    "    # 拼接并划分出数据与标签\n",
    "    data = pd.concat([normal_data_enr_df, interictal_data_enr_df, ictal_data_enr_df], ignore_index = True)\n",
    "    labels = pd.concat([normal_data_enr_lab, interictal_data_enr_lab, ictal_data_enr_lab], ignore_index = True)\n",
    "    \n",
    "    return data.values, labels.values\n",
    "\n",
    "# 对于RNN类的模型需要进行数据shape的转变\n",
    "def rnn_transform (raw_dat, frame_size = 1):\n",
    "    if raw_dat.shape[1] % frame_size == 0:\n",
    "        return raw_dat.reshape (raw_dat.shape[0], -1, frame_size)\n",
    "    else:\n",
    "        print (\"error frame_size\")\n",
    "        exit (1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据集拆分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal, interictal, ictal = load_data_as_df()\n",
    "\n",
    "normal_train, normal_test = train_test_split(normal, test_size = test_size)\n",
    "interictal_train, interictal_test = train_test_split(interictal, test_size = test_size)\n",
    "ictal_train, ictal_test = train_test_split(ictal, test_size = test_size)\n",
    "\n",
    "X_train, y_train = format_enrich_data (normal_train, interictal_train, ictal_train)\n",
    "X_test, y_test = format_enrich_data (normal_test, interictal_test, ictal_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 本项目设计的多种网络结构"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 双向LSTM模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Bi_LSTM(frame_size=16):\n",
    "    model = Sequential()\n",
    "    # frame=16 input_length=16\n",
    "    model.add(Bidirectional(LSTM(4 * frame_size, input_length=int(window_size / frame_size), input_dim=frame_size)))\n",
    "    model.add(Dropout(0.3))\n",
    "    # 独热是3+softmax，非独热是2+sigmoid\n",
    "    model.add(Dense(3, activation=\"softmax\"))\n",
    "\n",
    "    #独热是categorical_crossentropy，非独热是binary_crossentropy\n",
    "    model.compile('adam', 'categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = rnn_transform(X_train, frame_size=16)\n",
    "X_test = rnn_transform(X_test, frame_size=16)\n",
    "\n",
    "model = Bi_LSTM()\n",
    "monitor = ModelCheckpoint(\"./model/Bi_LSTM.h5\", monitor=\"val_accuracy\", save_best_only=True)\n",
    "history = model.fit(X_train, y_train, batch_size=100, epochs=25, validation_data=(X_test, y_test), callbacks=[monitor])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. GRU模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GRU_(frame_size=1):\n",
    "    input_shape, num_classes = (int(window_size / frame_size), frame_size), 3\n",
    "    n_hidden_units = 64\n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(16, 16, padding='valid', activation='relu', input_shape=input_shape))\n",
    "    model.add(Masking(mask_value=0.))\n",
    "    model.add(\n",
    "        GRU(units=n_hidden_units,\n",
    "            activation='relu',\n",
    "            kernel_initializer='orthogonal',\n",
    "            recurrent_initializer='orthogonal',\n",
    "            bias_initializer='zeros',\n",
    "            kernel_regularizer=regularizers.l2(0.01),\n",
    "            recurrent_regularizer=regularizers.l2(0.01),\n",
    "            bias_regularizer=None,\n",
    "            activity_regularizer=None,\n",
    "            kernel_constraint=None,\n",
    "            recurrent_constraint=None,\n",
    "            bias_constraint=None,\n",
    "            dropout=0.4,\n",
    "            recurrent_dropout=0.0,\n",
    "            implementation=1,\n",
    "            return_sequences=True,\n",
    "            return_state=False,\n",
    "            go_backwards=False,\n",
    "            stateful=False,\n",
    "            unroll=False))\n",
    "\n",
    "    model.add(\n",
    "        GRU(units=n_hidden_units,\n",
    "            activation='relu',\n",
    "            kernel_initializer='orthogonal',\n",
    "            recurrent_initializer='orthogonal',\n",
    "            bias_initializer='zeros',\n",
    "            kernel_regularizer=regularizers.l2(0.01),\n",
    "            recurrent_regularizer=regularizers.l2(0.01),\n",
    "            bias_regularizer=None,\n",
    "            activity_regularizer=None,\n",
    "            kernel_constraint=None,\n",
    "            recurrent_constraint=None,\n",
    "            bias_constraint=None,\n",
    "            dropout=0.3,\n",
    "            recurrent_dropout=0.0,\n",
    "            implementation=1,\n",
    "            return_sequences=False,\n",
    "            return_state=False,\n",
    "            go_backwards=False,\n",
    "            stateful=False,\n",
    "            unroll=False))\n",
    "\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = rnn_transform (X_train)\n",
    "X_test = rnn_transform (X_test)\n",
    "\n",
    "model = GRU_()\n",
    "monitor = ModelCheckpoint(filepath='./model/GRU.h5', monitor='val_accuracy', save_best_only=True, verbose=1)\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "history = model.fit(X_train, y_train, epochs=5, validation_data=(X_test, y_test), callbacks=[monitor])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.ANN模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ANN():\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Dense(100, activation='sigmoid'))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(Dense(20, activation='sigmoid'))\n",
    "    model.add(Dropout(0.1))\n",
    "\n",
    "    model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "    #adam = optimizers.Adam (lr=0.0001, beta_1=0.9, beta_2=0.999, epsilon=0.00001, decay=0.0, amsgrad=False)\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = format_enrich_data (normal_train, interictal_train, ictal_train)\n",
    "X_test, y_test = format_enrich_data (normal_test, interictal_test, ictal_test)\n",
    "\n",
    "model = ANN()\n",
    "checkpointer = ModelCheckpoint(filepath='./model/ANN.h5', verbose=0, monitor='val_accuracy', save_best_only=True)\n",
    "history = model.fit(X_train, y_train, batch_size=64, validation_data=(X_test, y_test), callbacks=[checkpointer], epochs=80, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. 单向LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Si_LSTM(frame_size=1):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(56, input_length=int(window_size / frame_size), input_dim=frame_size, return_sequences=True))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(LSTM(56))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(20))\n",
    "    model.add(Activation('tanh'))\n",
    "    model.add(Dense(3))\n",
    "    model.add(Activation('softmax'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = rnn_transform (X_train).astype('float64')\n",
    "X_test = rnn_transform (X_test).astype('float64')\n",
    "\n",
    "model = Si_LSTM()\n",
    "monitor = ModelCheckpoint(filepath='./model/Si_LSTM.h5', monitor='val_accuracy', save_best_only=True, verbose=1)\n",
    "hist = model.fit(((X_train[:, :] - X_train.mean()) / X_train.std()), y_train,\n",
    "                  validation_data=((X_test[:, :] - X_test.mean()) / X_test.std(), y_test),\n",
    "                  epochs=50,\n",
    "                  batch_size=15,\n",
    "                  shuffle=False,\n",
    "                  callbacks=[monitor])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.CNN模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CNN(input_shape=(256, 1), inception=True, res=True, strided=True, maxpool=False, avgpool=False, batchnorm=True):\n",
    "    config = {\n",
    "        # LSTM\n",
    "        'state_size': 32,\n",
    "\n",
    "        # CNN\n",
    "        'filters': 32,\n",
    "        'strides': 2,\n",
    "\n",
    "        # Output\n",
    "        'output_size': 3,\n",
    "\n",
    "        # Activations\n",
    "        'c_act': 'relu',\n",
    "        'r_act': 'hard_sigmoid',\n",
    "        'rk_act': 'tanh',\n",
    "        'batch_size': 512,\n",
    "        'learning_rate': 0.0012,\n",
    "        'epochs': 200,\n",
    "        'reg': 0.001,\n",
    "        'rec_drop': 0.32,\n",
    "        'drop': 0.5,\n",
    "        'cnn_drop': 0.6,\n",
    "    }\n",
    "    i = 0\n",
    "    pad = 'same'\n",
    "    padp = 'same'\n",
    "\n",
    "    c_act = config['c_act']\n",
    "    r_act = config['r_act']\n",
    "    rk_act = config['rk_act']\n",
    "\n",
    "    r = regularizers.l2(config['reg'])\n",
    "\n",
    "    input = Input(input_shape)\n",
    "    c = input\n",
    "    stride_size = config['strides'] if strided else 1\n",
    "    \n",
    "    # Inception\n",
    "    c0 = Conv1D(config['filters'], kernel_size=4, strides=stride_size, padding=pad, activation=c_act)(c)\n",
    "    c1 = Conv1D(config['filters'], kernel_size=8, strides=stride_size, padding=pad, activation=c_act)(c)\n",
    "    c2 = Conv1D(config['filters'], kernel_size=32, strides=stride_size, padding=pad, activation=c_act)(c)\n",
    "\n",
    "    c = concatenate([c0, c1, c2])\n",
    "\n",
    "    if maxpool:\n",
    "        c = MaxPooling1D(2, padding=padp)(c)\n",
    "    elif avgpool:\n",
    "        c = GlobalAveragePooling1D(2, padding=padp)(c)\n",
    "    if batchnorm:\n",
    "        c = BatchNormalization()(c)\n",
    "    c = SpatialDropout1D(config['cnn_drop'])(c)\n",
    "\n",
    "    c0 = Conv1D(config['filters'], kernel_size=4, strides=stride_size, padding=pad, activation=c_act)(c)\n",
    "    c1 = Conv1D(config['filters'], kernel_size=8, strides=stride_size, padding=pad, activation=c_act)(c)\n",
    "    c2 = Conv1D(config['filters'], kernel_size=32, strides=stride_size, padding=pad, activation=c_act)(c)\n",
    "\n",
    "    c = concatenate([c0, c1, c2])\n",
    "    if maxpool:\n",
    "        c = MaxPooling1D(2, padding=padp)(c)\n",
    "    elif avgpool:\n",
    "        c = GlobalAveragePooling1D(2, padding=padp)(c)\n",
    "    if batchnorm:\n",
    "        c = BatchNormalization()(c)\n",
    "    c = SpatialDropout1D(config['cnn_drop'])(c)\n",
    "\n",
    "    c0 = Conv1D(config['filters'], kernel_size=4, strides=stride_size, padding=pad, activation=c_act)(c)\n",
    "    c1 = Conv1D(config['filters'], kernel_size=8, strides=stride_size, padding=pad, activation=c_act)(c)\n",
    "    c2 = Conv1D(config['filters'], kernel_size=32, strides=stride_size, padding=pad, activation=c_act)(c)\n",
    "\n",
    "    c = concatenate([c0, c1, c2])\n",
    "    if maxpool:\n",
    "        c = MaxPooling1D(2, padding=padp)(c)\n",
    "    elif avgpool:\n",
    "        c = GlobalAveragePooling1D(2, padding=padp)(c)\n",
    "    if batchnorm:\n",
    "        c = BatchNormalization()(c)\n",
    "    c = SpatialDropout1D(config['cnn_drop'])(c)\n",
    "    \n",
    "    # Residual RNN\n",
    "    g1 = GRU(config['state_size'],\n",
    "             return_sequences=True,\n",
    "             activation=rk_act,\n",
    "             recurrent_activation=r_act,\n",
    "             dropout=config['rec_drop'],\n",
    "             recurrent_dropout=config['rec_drop'],\n",
    "             recurrent_regularizer=r,\n",
    "             kernel_regularizer=r)(c)\n",
    "    g2 = GRU(config['state_size'],\n",
    "             return_sequences=True,\n",
    "             activation=rk_act,\n",
    "             recurrent_activation=r_act,\n",
    "             dropout=config['rec_drop'],\n",
    "             recurrent_dropout=config['rec_drop'],\n",
    "             recurrent_regularizer=r,\n",
    "             kernel_regularizer=r)(g1)\n",
    "    g_concat1 = concatenate([g1, g2])\n",
    "\n",
    "    g3 = GRU(config['state_size'],\n",
    "             return_sequences=True,\n",
    "             activation=rk_act,\n",
    "             recurrent_activation=r_act,\n",
    "             dropout=config['rec_drop'],\n",
    "             recurrent_dropout=config['rec_drop'],\n",
    "             recurrent_regularizer=r,\n",
    "             kernel_regularizer=r)(g_concat1)\n",
    "    g_concat2 = concatenate([g1, g2, g3])\n",
    "\n",
    "    g = GRU(config['state_size'],\n",
    "            return_sequences=False,\n",
    "            activation=rk_act,\n",
    "            recurrent_activation=r_act,\n",
    "            dropout=config['rec_drop'],\n",
    "            recurrent_dropout=config['rec_drop'],\n",
    "            recurrent_regularizer=r,\n",
    "            kernel_regularizer=r)(g_concat2)\n",
    "    d = Dense(config['output_size'])(g)\n",
    "    out = Softmax()(d)\n",
    "\n",
    "    model = Model(input, out)\n",
    "\n",
    "    adam = optimizers.Adam(lr=0.00002, beta_1=0.9, beta_2=0.999, epsilon=0.00000001, decay=0.0, amsgrad=False)\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNN(input_shape=(256,1),inception=True, res=True, strided=True, maxpool=False, avgpool=False, batchnorm=True)\n",
    "checkpointer = ModelCheckpoint(filepath='./model/CNN.h5', verbose=0, monitor='val_accuracy', save_best_only=True)\n",
    "history = model.fit(X_train, y_train, batch_size=64, validation_data=(X_test, y_test), callbacks=[checkpointer], epochs=150, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 结果展示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_result(model):\n",
    "    result = model.predict(X_test)\n",
    "    test_num = result.shape[0]\n",
    "    result_sort = np.argsort(result, axis=1)\n",
    "    right, ictal_r, pre_ictal, ictal_n = 0, 0, 0, 0\n",
    "    for i in range(test_num):\n",
    "        if result_sort[i][-1] == 2:\n",
    "            pre_ictal += 1\n",
    "            if result_sort[i][-1] == list(y_test[i]).index(1):\n",
    "                ictal_r += 1\n",
    "        if list(y_test[i]).index(1) == 2:\n",
    "            ictal_n += 1\n",
    "        if result_sort[i][-1] == list(y_test[i]).index(1):\n",
    "            right += 1\n",
    "    print(\"acc: %.4f\" % (right / test_num)) # 准确率\n",
    "    print(\"sen: %.4f\" % (ictal_r / ictal_n)) # 敏感性\n",
    "    print(\"recall: %.4f\" % (ictal_r / pre_ictal)) # 召回率\n",
    "    \n",
    "cal_result(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对accuracy和val_accuracy作图\n",
    "def plot_accuracy(epoch, val_accuracy, accuracy):\n",
    "    best_epoch = 0\n",
    "    max_accuracy = 0\n",
    "    for i in range(len(epoch)):\n",
    "        if val_accuracy[i] > max_accuracy:\n",
    "            max_accuracy = val_accuracy[i]\n",
    "            best_epoch = i\n",
    "    plt.plot(epoch, val_accuracy, label='Validation')\n",
    "    plt.plot(epoch, accuracy, label='Training')\n",
    "    plt.axvline(x=epoch[best_epoch], color=\"r\", ls=\"--\", label='Best Epoch')\n",
    "    plt.legend()\n",
    "    # plt.ylim(0.4, 1)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Accuracy of prediction results')\n",
    "    plt.text(best_epoch, max_accuracy + 0.01, '%.4f' %max_accuracy, ha='center', va='bottom', fontsize=12) # 最高val_accuracy对应epoch\n",
    "    plt.show()\n",
    "    print(\"The highest validation accuracy is %.4f\" % max_accuracy)\n",
    "\n",
    "# 对loss和val_loss作图\n",
    "def plot_loss(epoch, val_loss, loss):\n",
    "    best_epoch = 0\n",
    "    min_loss = 1\n",
    "    for i in range(len(epoch)):\n",
    "        if val_loss[i] < min_loss:\n",
    "            min_loss = val_loss[i]\n",
    "            best_epoch = i\n",
    "    plt.plot(epoch, val_loss, label='Validation')\n",
    "    plt.plot(epoch, loss, label='Training')\n",
    "    plt.axvline(x=epoch[best_epoch], color=\"r\", ls=\"--\", label='Best Epoch')\n",
    "    plt.legend()\n",
    "    # plt.ylim(0, 1.2)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Loss of prediction results')\n",
    "    plt.text(best_epoch, min_loss+0.1, '%.4f' % min_loss, ha='center', va='bottom', fontsize=12) # 最低val_loss对应epoch\n",
    "    plt.show()\n",
    "    print(\"The lowest validation loss is %.4f\" % min_loss)\n",
    "\n",
    "plot_accuracy(history.epoch, history.history['val_acc'], history2.history['acc'])\n",
    "plot_loss(history.epoch, history.history['val_loss'], history2.history['loss'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "metadata": {
   "interpreter": {
    "hash": "2a9e50a52e033ddfe4619f67a40249d6fca5daaa85638421e6bcbc08d1d61874"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
