{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notebook based off of https://arxiv.org/pdf/1801.05412.pdf , refer to this for parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import of necesary librairies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential, Model, Input, load_model\n",
    "from keras.layers import Input, Dense, Dropout\n",
    "from keras.layers import Embedding, Activation, Flatten\n",
    "from keras.layers import Conv1D, GlobalAveragePooling1D, MaxPooling1D, BatchNormalization, LSTM\n",
    "from keras.utils import to_categorical\n",
    "from keras import optimizers\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.utils import np_utils\n",
    "import time\n",
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML\n",
    "from keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utilitaries functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def folder_to_df(letter): #import the .txt files\n",
    "    full_path =\"data/bonn_uni_datasets/\"+ letter + \"/*.*\"\n",
    "    files = files = glob.glob(full_path)\n",
    "    df_list = []\n",
    "    for file in files:\n",
    "        df_list.append(pd.read_csv(file, header = None))\n",
    "    big_df = pd.concat(df_list, ignore_index=True, axis= 1)#垂直方向进行拼接\n",
    "    return big_df.T #每一列是一个20s序列\n",
    "\n",
    "def norm(X): # zero mean and unit variance normalization\n",
    "    X = X - np.mean(X)\n",
    "    X = X / np.std(X)\n",
    "    return X\n",
    "\n",
    "def window(a, w = 512, o = 64, copy = False): #window sliding function\n",
    "    #default for training, for testing data we will split each signal in four of 1024 and apply\n",
    "    #a window size of 512 with a stride (o) of 256\n",
    "    sh = (a.size - w + 1, w)\n",
    "    st = a.strides * 2\n",
    "    view = np.lib.stride_tricks.as_strided(a, strides = st, shape = sh)[0::o]\n",
    "    if copy:\n",
    "        return view.copy()\n",
    "    else:\n",
    "        return view\n",
    "\n",
    "def enrich_train(df): #enrich data by splicing the 4097-long signals \n",
    "    #into 512 long ones with a stride of 64\n",
    "    labels = df.iloc[:,-1]\n",
    "    data = df.iloc[:, :-1]\n",
    "    res = list()\n",
    "    for i in range(len(data)):\n",
    "        res += [window(data.iloc[i].values)]\n",
    "    return res\n",
    "\n",
    "def reshape_x(arr): #shape the input data into the correct form (x1,x2,1)\n",
    "    nrows = arr.shape[0]\n",
    "    ncols = arr.shape[1]\n",
    "    return arr.reshape(nrows, ncols, 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data into dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_as_df():\n",
    "    A = norm(folder_to_df('A'))\n",
    "    B = norm(folder_to_df('B'))\n",
    "    C = norm(folder_to_df('C'))\n",
    "    D = norm(folder_to_df('D'))\n",
    "    E = norm(folder_to_df('E'))\n",
    "    \n",
    "    normal = A.append(B).reset_index(drop = True)\n",
    "    interictal = C.append(D).reset_index(drop = True)\n",
    "    ictal = E\n",
    "\n",
    "    return normal, interictal, ictal\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split into 90%/10%, keeping the 10% for the testing of the majority voting later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal, interictal, ictal = load_data_as_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(180, 4097)\n"
     ]
    }
   ],
   "source": [
    "normal_train, normal_vote = train_test_split(normal, test_size = 0.1)\n",
    "interictal_train, interictal_vote = train_test_split(interictal, test_size = 0.1)\n",
    "ictal_train, ictal_vote = train_test_split(ictal, test_size = 0.1)\n",
    "print(normal_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enriching the data as per Scheme 1 in the paper\n",
    "\n",
    "### window sliding with a stride of 64 and length of 512, as well as adding labels and format into the correct shape for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_enrich_train(normal, interictal, ictal):\n",
    "    \n",
    "    #enrich data and reshape it to have a two dimensional array instead of three\n",
    "    normal_train_enr = np.asarray(enrich_train(normal)).reshape(-1, np.asarray(enrich_train(normal)).shape[-1])\n",
    "    interictal_train_enr = np.asarray(enrich_train(interictal)).reshape(-1, np.asarray(enrich_train(interictal)).shape[-1])\n",
    "    ictal_train_enr = np.asarray(enrich_train(ictal)).reshape(-1, np.asarray(enrich_train(ictal)).shape[-1])\n",
    "\n",
    "    #change into a dataframe to add labels easily\n",
    "    normal_train_enr_df = pd.DataFrame(normal_train_enr)\n",
    "    interictal_train_enr_df = pd.DataFrame(interictal_train_enr)\n",
    "    ictal_train_enr_df = pd.DataFrame(ictal_train_enr)\n",
    "    \n",
    "    normal_train_enr_df['labels'] = 0 # normal\n",
    "    interictal_train_enr_df['labels'] = 1 #interictal\n",
    "    ictal_train_enr_df['labels'] = 2 #ictal\n",
    "\n",
    "    #concat all\n",
    "    data_labels = pd.concat([normal_train_enr_df, interictal_train_enr_df, ictal_train_enr_df], ignore_index = True)\n",
    "    \n",
    "\n",
    "    #separates data and labels into numpy arrays for keras\n",
    "    data = data_labels.drop('labels', axis = 1).values\n",
    "    labels = data_labels.labels.values\n",
    "    \n",
    "    #labels = np.expand_dims(labels, axis=1)\n",
    "    \n",
    "    return data, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The model, as per :\n",
    "![Schema of the model](images/model_schema.png)\n",
    "\n",
    "\n",
    "### Parameters taken in the paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(,inception=True, res=True, strided=True, maxpool=False, avgpool=False, batchnorm=True):\n",
    "# #     X = Sequential()\n",
    "# #     X_input = Input(input_shape)\n",
    "# #     X = ZeroPadding2D((3, 3))(X_input)\n",
    "# #     X_shortcut = X\n",
    "#     model = Sequential()\n",
    "#     #Conv - 1\n",
    "#     model.add(Conv1D(24, 5,strides =  3, input_shape=(512,1)))\n",
    "#     model.add(BatchNormalization())\n",
    "#     model.add(Activation('relu'))\n",
    "# #     X = Conv1D(24, 5,strides =  3, input_shape=(512,1))\n",
    "# #     X = BatchNormalization(X)\n",
    "# #     X = Activation('relu')(X)\n",
    "#     #Conv - 2\n",
    "#     model.add(Conv1D(16, 3,strides =  2))\n",
    "#     model.add(BatchNormalization())\n",
    "#     model.add(Activation('relu'))\n",
    "# #     X = Conv1D(16, 3,strides =  3, input_shape=(512,1))\n",
    "# #     X = BatchNormalization(X)\n",
    "# #     X = Activation('relu')(X)\n",
    "#     #Conv - 3\n",
    "#     model.add(Conv1D(8, 3,strides =  2))\n",
    "#     model.add(BatchNormalization())\n",
    "#     model.add(Activation('relu'))\n",
    "# #     X = Conv1D(8, 3,strides =  3, input_shape=(512,1))\n",
    "# #     X = BatchNormalization(X)\n",
    "# #     X = Activation('relu')(X)\n",
    "#     model.add(Conv1D(4, 3,strides =  2))\n",
    "#     model.add(BatchNormalization())\n",
    "#     model.add(Activation('relu'))\n",
    "#     #FC -1\n",
    "#     model.add(Flatten())\n",
    "#     model.add(Dense(20))\n",
    "#     model.add(Activation('relu'))\n",
    "# #     X = Flatten(X)\n",
    "# #     X = Dense(20)(X)\n",
    "# #     X = Activation('relu')(X)\n",
    "#     #Dropout\n",
    "#     model.add(Dropout(0.5))\n",
    "# #     X = Dropout(0.5)(X)\n",
    "#     #FC -2\n",
    "#     model.add(Dense(units=50, activation='relu',\n",
    "#                 use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros',))\n",
    "#     model.add(Dense(3,activation = 'softmax'))\n",
    "# #     X = Dense(3,activation = 'softmax')(X)\n",
    "#     #softmax\n",
    "# #     model.add(Activation('softmax'))\n",
    "        self.i = 0\n",
    "        pad = 'same'\n",
    "        padp = 'same'\n",
    "\n",
    "        c_act = config['c_act']\n",
    "        r_act = config['r_act']\n",
    "        rk_act = config['rk_act']\n",
    "\n",
    "        r = kr.regularizers.l2(config['reg'])\n",
    "\n",
    "        c = self.input\n",
    "        stride_size = config['strides'] if strided else 1\n",
    "\n",
    "        if inception:\n",
    "            c0 = layers.Conv1D(config['filters'], kernel_size=4, strides=stride_size, padding=pad,\n",
    "                               activation=c_act)(c)\n",
    "            c1 = layers.Conv1D(config['filters'], kernel_size=8, strides=stride_size, padding=pad,\n",
    "                               activation=c_act)(c)\n",
    "            c2 = layers.Conv1D(config['filters'], kernel_size=32, strides=stride_size, padding=pad,\n",
    "                               activation=c_act)(c)\n",
    "\n",
    "            c = layers.concatenate([c0, c1, c2])\n",
    "\n",
    "            if maxpool:\n",
    "                c = layers.MaxPooling1D(2, padding=padp)(c)\n",
    "            elif avgpool:\n",
    "                c = layers.AveragePooling1D(2, padding=padp)(c) \n",
    "            if batchnorm:\n",
    "                c = layers.BatchNormalization()(c)\n",
    "            c = layers.SpatialDropout1D(self.config['cnn_drop'])(c)\n",
    "\n",
    "\n",
    "\n",
    "            c0 = layers.Conv1D(config['filters'], kernel_size=4, strides=stride_size, padding=pad,\n",
    "                               activation=c_act)(c)\n",
    "            c1 = layers.Conv1D(config['filters'], kernel_size=8, strides=stride_size, padding=pad,\n",
    "                               activation=c_act)(c)\n",
    "            c2 = layers.Conv1D(config['filters'], kernel_size=32, strides=stride_size, padding=pad,\n",
    "                               activation=c_act)(c)\n",
    "\n",
    "            c = layers.concatenate([c0, c1, c2])\n",
    "            if maxpool:\n",
    "                c = layers.MaxPooling1D(2, padding=padp)(c)\n",
    "            elif avgpool:\n",
    "                c = layers.AveragePooling1D(2, padding=padp)(c) \n",
    "            if batchnorm:\n",
    "                c = layers.BatchNormalization()(c)\n",
    "            c = layers.SpatialDropout1D(self.config['cnn_drop'])(c)\n",
    "\n",
    "            c0 = layers.Conv1D(config['filters'], kernel_size=4, strides=stride_size, padding=pad,\n",
    "                               activation=c_act)(c)\n",
    "            c1 = layers.Conv1D(config['filters'], kernel_size=8, strides=stride_size, padding=pad,\n",
    "                               activation=c_act)(c)\n",
    "            c2 = layers.Conv1D(config['filters'], kernel_size=32, strides=stride_size, padding=pad,\n",
    "                               activation=c_act)(c)\n",
    "\n",
    "            c = layers.concatenate([c0, c1, c2])\n",
    "            if maxpool:\n",
    "                c = layers.MaxPooling1D(2, padding=padp)(c)\n",
    "            elif avgpool:\n",
    "                c = layers.AveragePooling1D(2, padding=padp)(c) \n",
    "            if batchnorm:\n",
    "                c = layers.BatchNormalization()(c)\n",
    "            c = layers.SpatialDropout1D(config['cnn_drop'])(c)\n",
    "\n",
    "        else:   # No inception Modules\n",
    "            c = layers.Conv1D(config['filters'], kernel_size=4, strides=stride_size, padding=pad, activation=c_act)(input)\n",
    "            if maxpool:\n",
    "                c = layers.MaxPooling1D(2, padding=padp)(c)\n",
    "            elif avgpool:\n",
    "                c = layers.AveragePooling1D(2, padding=padp)(c) \n",
    "            if batchnorm:\n",
    "                c = layers.BatchNormalization()(c)\n",
    "            c = layers.SpatialDropout1D(config['cnn_drop'])(c)\n",
    "\n",
    "            c = layers.Conv1D(config['filters'], kernel_size=4, strides=stride_size, padding=pad, activation=c_act)(c)\n",
    "            if maxpool:\n",
    "                c = layers.MaxPooling1D(2, padding=padp)(c)\n",
    "            elif avgpool:\n",
    "                c = layers.AveragePooling1D(2, padding=padp)(c) \n",
    "            if batchnorm:\n",
    "                c = layers.BatchNormalization()(c)\n",
    "            c = layers.SpatialDropout1D(config['cnn_drop'])(c)\n",
    "\n",
    "            c = layers.Conv1D(config['filters'], kernel_size=4, strides=stride_size, padding=pad, activation=c_act)(c)\n",
    "            if maxpool:\n",
    "                c = layers.MaxPooling1D(2, padding=padp)(c)\n",
    "            elif avgpool:\n",
    "                c = layers.AveragePooling1D(2, padding=padp)(c) \n",
    "            if batchnorm:\n",
    "                c = layers.BatchNormalization()(c)\n",
    "            c = layers.SpatialDropout1D(self.config['cnn_drop'])(c)\n",
    "\n",
    "        if res: # Residual RNN\n",
    "            g1 = layers.GRU(config['state_size'], return_sequences=True, activation=rk_act, recurrent_activation=r_act,\n",
    "                            dropout=config['rec_drop'], recurrent_dropout=config['rec_drop'],\n",
    "                            recurrent_regularizer=r, kernel_regularizer=r)(c)\n",
    "            g2 = layers.GRU(config['state_size'], return_sequences=True,  activation=rk_act, recurrent_activation=r_act,\n",
    "                            dropout=config['rec_drop'], recurrent_dropout=config['rec_drop'],\n",
    "                            recurrent_regularizer=r, kernel_regularizer=r)(g1)\n",
    "            g_concat1 = layers.concatenate([g1, g2])\n",
    "\n",
    "            g3 = layers.GRU(self.config['state_size'], return_sequences=True, activation=rk_act, recurrent_activation=r_act,\n",
    "                            dropout=self.config['rec_drop'], recurrent_dropout=self.config['rec_drop'],\n",
    "                            recurrent_regularizer=r, kernel_regularizer=r)(g_concat1)\n",
    "            g_concat2 = layers.concatenate([g1, g2, g3])\n",
    "\n",
    "            g = layers.GRU(self.config['state_size'], return_sequences=False, activation=rk_act, recurrent_activation=r_act,\n",
    "                           dropout=self.config['rec_drop'], recurrent_dropout=self.config['rec_drop'],\n",
    "                            recurrent_regularizer=r, kernel_regularizer=r)(g_concat2)\n",
    "\n",
    "        else:   # No Residual RNN\n",
    "            g = layers.GRU(self.config['state_size'], return_sequences=True, activation=rk_act, recurrent_activation=r_act,\n",
    "                           dropout=self.config['rec_drop'], recurrent_dropout=self.config['rec_drop'],\n",
    "                            recurrent_regularizer=r, kernel_regularizer=r)(c)\n",
    "\n",
    "            g = layers.GRU(self.config['state_size'], return_sequences=True, activation=rk_act, recurrent_activation=r_act,\n",
    "                           dropout=self.config['rec_drop'], recurrent_dropout=self.config['rec_drop'],\n",
    "                            recurrent_regularizer=r, kernel_regularizer=r)(g)\n",
    "            g = layers.GRU(self.config['state_size'], return_sequences=True, activation=rk_act, recurrent_activation=r_act,\n",
    "                           dropout=self.config['rec_drop'], recurrent_dropout=self.config['rec_drop'],\n",
    "                            recurrent_regularizer=r, kernel_regularizer=r)(g)\n",
    "\n",
    "            g = layers.GRU(self.config['state_size'], return_sequences=False, activation=rk_act, recurrent_activation=r_act,\n",
    "                           dropout=self.config['rec_drop'], recurrent_dropout=self.config['rec_drop'],\n",
    "                            recurrent_regularizer=r, kernel_regularizer=r)(g)\n",
    "\n",
    "\n",
    "        d = layers.Dense(self.config['output_size'])(g)\n",
    "        out = layers.Softmax()(d)\n",
    "\n",
    "        self.model = Model(self.input, out)\n",
    "        print(\"{} initialized.\".format(self.model.name))\n",
    "\n",
    "\n",
    "#     adam = optimizers.Adam(lr=0.00002, beta_1=0.9, beta_2=0.999, epsilon=0.00000001, decay=0.0, amsgrad=False)\n",
    "\n",
    "#     model.compile(loss='categorical_crossentropy',\n",
    "#                   optimizer=adam,\n",
    "#                   metrics=['accuracy'])\n",
    "#     return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training function as well as the stratified 10 fold cross validation for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_evaluate_model(model, xtrain, ytrain, xval, yval, fold):\n",
    "    model_name = 'P-1D-CNN'\n",
    "    checkpointer = ModelCheckpoint(filepath='checkpoints/'+'fold'+ str(fold)+'.'+model_name + '.{epoch:03d}-{accuracy:.3f}.h5',verbose=0,monitor ='acc', save_best_only=True)\n",
    "    history = model.fit(xtrain, ytrain, batch_size=32, callbacks = [checkpointer],epochs=100, verbose = 1)\n",
    "    print(history)\n",
    "    score = model.evaluate(xval, yval, batch_size=32)\n",
    "    print('\\n')\n",
    "    print(score)\n",
    "    return score, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Fold 1 / 2\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "create_model() missing 1 required positional argument: 'self'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-06da48917070>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;31m# Clearing the NN.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m     \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minception\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstrided\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmaxpool\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mavgpool\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatchnorm\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m     \u001b[0mscore\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_evaluate_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mxtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mytrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mxval\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0myval\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Ran \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"/\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_folds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Fold in %s seconds ---\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: create_model() missing 1 required positional argument: 'self'"
     ]
    }
   ],
   "source": [
    "n_folds = 2\n",
    "X, y = format_enrich_train(normal_train, interictal_train, ictal_train)\n",
    "#initialize 10 fold validation\n",
    "skf = StratifiedKFold(n_splits=2, shuffle=True)\n",
    "\n",
    "\n",
    "#10 fold cross validation loop\n",
    "for i, (train, test) in enumerate(skf.split(X,y)):\n",
    "    print(\"Running Fold\", i+1, \"/\", n_folds)\n",
    "    start_time = time.time()\n",
    "    X = reshape_x(X)\n",
    "    xtrain, xval = X[train], X[test]\n",
    "    ytrain, yval = y[train], y[test]\n",
    "    ytrain = to_categorical(ytrain, num_classes=3, dtype='float32')\n",
    "    yval = to_categorical(yval, num_classes=3, dtype='float32')\n",
    "\n",
    "\n",
    "    model = None # Clearing the NN.\n",
    "    model = create_model(inception=True, res=True, strided=True, maxpool=False, avgpool=False, batchnorm=True)\n",
    "    score, history = train_evaluate_model(model, xtrain, ytrain, xval, yval, i+1)\n",
    "    print(\"Ran \", i+1, \"/\", n_folds, \"Fold in %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = load_model('best_model.0.966.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additional necessary funtions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_vote(df):\n",
    "    res = list()\n",
    "    for i in range(len(df)):\n",
    "        res += [window(df.iloc[i].values,w= 512, o = 64)]\n",
    "    return np.asarray(res)\n",
    "\n",
    "def count_votes(my_list): \n",
    "    freq = {} \n",
    "    for i in my_list: \n",
    "        if (i in freq): \n",
    "            freq[i] += 1\n",
    "        else: \n",
    "            freq[i] = 1\n",
    "    return freq\n",
    "\n",
    "def reshape_signal(signal):\n",
    "    signal = np.expand_dims(signal, axis=1)\n",
    "    signal = np.expand_dims(signal, axis=0)\n",
    "    return np.asarray(signal)\n",
    "\n",
    "def evaluate_subsignals(subsignals,model):\n",
    "    vote_list = np.array([])\n",
    "    for i in range(len(subsignals)):\n",
    "        mini_signal = reshape_signal(subsignals[i])\n",
    "        ynew = model.predict_classes(mini_signal)\n",
    "        vote_list = np.append(vote_list, ynew)\n",
    "    decision = count_votes(vote_list)\n",
    "    return decision_to_str(decision), vote_list\n",
    "\n",
    "def decision_to_str(dec):\n",
    "    res = list()\n",
    "    for key,val in dec.items():\n",
    "        if key == 0:\n",
    "            res += ['normal: ' + str(val) + ' votes' + '\\n']\n",
    "        if key == 1:\n",
    "            res += ['ictal: ' + str(val) + ' votes' + '\\n']\n",
    "        if key == 2:\n",
    "            res += ['interictal: ' + str(val) + ' votes' + '\\n']\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exctracting 1st normal signal for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "big_signal = split_vote(ictal_vote)\n",
    "subsignals = big_signal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is divided into 15 subsignals of length 512, the model will \"vote\" on each subsignal and decide by majority"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for subsignal in subsignals:\n",
    "    decision, vote_list = evaluate_subsignals(subsignal,best_model)\n",
    "    print(vote_list)\n",
    "    print(decision[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
