{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import keras as kr\n",
    "\n",
    "import data\n",
    "import net\n",
    "\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Force new Pre-processing\n",
    "pre_process = False\n",
    "if not pre_process: # First try to load old data\n",
    "    if data.check_load():\n",
    "        print(\"Loading pre-processed data\")\n",
    "        X_train, y_train, X_val, y_val, X_test, y_test = data.load_pre_processed()\n",
    "    else:\n",
    "        pre_process = True\n",
    "if pre_process: # Load raw data\n",
    "    print(\"Loading raw Data\")\n",
    "    X = []\n",
    "    y = []\n",
    "    Xt = []\n",
    "    yt = []\n",
    "\n",
    "    s0 = []\n",
    "    sv0 = []\n",
    "\n",
    "    n_s = 22\n",
    "    n_test = 50\n",
    "\n",
    "    for i in range(1, 10):\n",
    "        xi, yi = data.load(i)\n",
    "        yi = data.recode_y(yi)\n",
    "\n",
    "        xi, yi = data.drop_nan(xi, yi)\n",
    "\n",
    "        xi, yi = data.shuffle(xi[:, :n_s], yi)\n",
    "\n",
    "        yi = kr.utils.to_categorical(yi)\n",
    "\n",
    "        Xt.append(xi[:n_test])\n",
    "        yt.append(yi[:n_test])\n",
    "\n",
    "        X.append(xi[n_test:])\n",
    "        y.append(yi[n_test:])\n",
    "        print(\"Shape %d:\" % i, X[-1].shape)\n",
    "\n",
    "print(\"Data Loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if pre_process:\n",
    "    mean = []\n",
    "    var = []\n",
    "    X_train = None\n",
    "    y_train = None\n",
    "\n",
    "    X_test = None\n",
    "    y_test = None\n",
    "\n",
    "    X_val = None\n",
    "    y_val = None\n",
    "\n",
    "    X_train = np.concatenate(X, axis=0)\n",
    "    X_test = np.concatenate(Xt, axis=0)\n",
    "\n",
    "    y_train = np.concatenate(y, axis=0)\n",
    "    y_test = np.concatenate(yt, axis=0)\n",
    "    print(\"Data merged\")\n",
    "\n",
    "    # Butterworth Filter\n",
    "    butt = 1\n",
    "    if butt:\n",
    "        X_train = data.butter_band(X_train, 4, 120)\n",
    "        X_test = data.butter_band(X_test, 4, 120)\n",
    "        print(\"Butter filter applied\")\n",
    "\n",
    "    \n",
    "    # Normalize Data\n",
    "    ax = (0, 2)\n",
    "    X_train, m, v = data.normalize_data(X_train, None, None, axis=ax)\n",
    "    X_test, _, _ = data.normalize_data(X_test, m, v)#, None, None, axis=ax)\n",
    "\n",
    "\n",
    "    # Make validation set\n",
    "    X_train, y_train = data.shuffle(X_train, y_train)\n",
    "    val_split = 0.2\n",
    "    num_val = int(val_split*(y_train.shape[0]))\n",
    "    X_val = X_train[:num_val]\n",
    "    X_train = X_train[num_val:]\n",
    "    y_val = y_train[:num_val]\n",
    "    y_train = y_train[num_val:]\n",
    "    print(\"Data standardized\")\n",
    "\n",
    "    # Window the data\n",
    "    window = 500\n",
    "    stride = 50\n",
    "    start = 0\n",
    "\n",
    "    X_train, y_train = data.windowing(X_train, y_train, start, window, stride=stride)\n",
    "    X_test, y_test = data.windowing(X_test, y_test, start, window, stride=stride)\n",
    "    # Validation cut\n",
    "    X_val, y_val = data.windowing(X_val, y_val, start, window, stride=stride)\n",
    "    print(\"Windowing applied: w {}; s {}\".format(window, stride))\n",
    "\n",
    "    \n",
    "    # Noise augmentation\n",
    "    noise = 0\n",
    "    if noise > 0:\n",
    "        X_train, y_train = data.augment_noise(X_train, y_train, p=noise)\n",
    "        print(\"Noise augmented\")\n",
    "\n",
    "    \n",
    "    # Swap axes to match RNN\n",
    "    X_train = data.swap_axis(X_train)\n",
    "    X_test = data.swap_axis(X_test)\n",
    "    X_val = data.swap_axis(X_val)\n",
    "    \n",
    "    data.save_pre_processed(X_train, y_train, X_val, y_val, X_test, y_test)\n",
    "    print(\"Preprocessing complete. Train shape: {}\".format(X_train.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Three models are tested:\n",
    "#### 1. Stride of 2 on 1D convolutions\n",
    "#### 2. Stride of 1 and use max pools\n",
    "#### 3. Stride of 1 and use average pools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RNNconfig = {\n",
    "    'num_steps' : X_train.shape[1],\n",
    "    'sensors' : X_train.shape[2],\n",
    "    # LSTM\n",
    "    'state_size' : 32,\n",
    "\n",
    "    # CNN\n",
    "    'filters' : 32,\n",
    "    'strides' : 2,\n",
    "\t\n",
    "    # Output\n",
    "    'output_size' : 4,\n",
    "    \n",
    "    # Activations\n",
    "    'c_act' : 'relu',\n",
    "    'r_act' : 'hard_sigmoid',\n",
    "    'rk_act' : 'tanh',\n",
    "    \n",
    "    'batch_size' : 512,\n",
    "    'learning_rate' : 0.0012,\n",
    "    'epochs' : 200,\n",
    "    'reg' : 0.001,\n",
    "    \n",
    "    'rec_drop': 0.32,\n",
    "    'drop' : 0.5,\n",
    "    'cnn_drop' : 0.6,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model 1: Run RNN with stride of 2 on convs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "RNN_stride = net.RNN(RNNconfig)\n",
    "\n",
    "RNN_stride.build_model(inception=True, res=True, strided=True, maxpool=False, avgpool=False, batchnorm=True)\n",
    "\n",
    "m = RNN_stride.model\n",
    "# print(m.summary())\n",
    "\n",
    "tic = time.time()\n",
    "\n",
    "history_stride = RNN_stride.train(X_train, y_train, X_val, y_val, verbose=1)\n",
    "acc = RNN_stride.eval_acc(X_test, y_test)\n",
    "\n",
    "print(\"\\tTraining time: {}s\".format(time.time()-tic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### Plot History\n",
    "plt.plot(history_stride.history['acc'])\n",
    "plt.plot(history_stride.history['val_acc'])\n",
    "plt.title('model accuracy: stride of 2')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# Loss\n",
    "plt.plot(history_stride.history['loss'])\n",
    "plt.plot(history_stride.history['val_loss'])\n",
    "plt.title('model loss: stride of 2')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper right')\n",
    "plt.show()\n",
    "\n",
    "# Stats\n",
    "print(\"Max val Acc\", np.max(history_stride.history['val_acc']))\n",
    "print(\"Min val loss\", np.min(history_stride.history['val_loss']))\n",
    "print(\"Test Accuracy: {}\".format(acc))\n",
    "print(\"Max train Acc\", np.max(history_stride.history['acc']))\n",
    "\n",
    "# Save model\n",
    "RNN_max.save_model(\"model_stride.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model 2: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RNN_max = net.RNN(RNNconfig)\n",
    "\n",
    "RNN_max.build_model(inception=True, res=True, strided=False, maxpool=True, avgpool=False, batchnorm=True)\n",
    "\n",
    "m = RNN_max.model\n",
    "# print(m.summary())\n",
    "\n",
    "tic = time.time()\n",
    "\n",
    "history_max = RNN_max.train(X_train, y_train, X_val, y_val, verbose=1)\n",
    "acc = RNN_max.eval_acc(X_test, y_test)\n",
    "\n",
    "print(\"\\tTraining time: {}s\".format(time.time()-tic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Plot History\n",
    "plt.plot(history_max.history['acc'])\n",
    "plt.plot(history_max.history['val_acc'])\n",
    "plt.title('model accuracy: max pool')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# Loss\n",
    "plt.plot(history_max.history['loss'])\n",
    "plt.plot(history_max.history['val_loss'])\n",
    "plt.title('model loss: max pool')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper right')\n",
    "plt.show()\n",
    "\n",
    "# Stats\n",
    "print(\"Max val Acc\", np.max(history_max.history['val_acc']))\n",
    "print(\"Min val loss\", np.min(history_max.history['val_loss']))\n",
    "print(\"Test Accuracy: {}\".format(acc))\n",
    "print(\"Max train Acc\", np.max(history_max.history['acc']))\n",
    "\n",
    "# Save model\n",
    "RNN_max.save_model(\"model_max.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model 3: Average pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RNN_avg = net.RNN(RNNconfig)\n",
    "\n",
    "RNN_avg.build_model(inception=True, res=True, strided=False, maxpool=False, avgpool=True, batchnorm=True)\n",
    "\n",
    "m = RNN_avg.model\n",
    "# print(m.summary())\n",
    "\n",
    "tic = time.time()\n",
    "\n",
    "history_avg = RNN_avg.train(X_train, y_train, X_val, y_val, verbose=1)\n",
    "acc = RNN_avg.eval_acc(X_test, y_test)\n",
    "\n",
    "print(\"\\tTraining time: {}s\".format(time.time()-tic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Plot History\n",
    "plt.plot(history_avg.history['acc'])\n",
    "plt.plot(history_avg.history['val_acc'])\n",
    "plt.title('model accuracy: average pool')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# Loss\n",
    "plt.plot(history_avg.history['loss'])\n",
    "plt.plot(history_avg.history['val_loss'])\n",
    "plt.title('model loss: average pool')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper right')\n",
    "plt.show()\n",
    "\n",
    "# Stats\n",
    "print(\"Max val Acc\", np.max(history_avg.history['val_acc']))\n",
    "print(\"Min val loss\", np.min(history_avg.history['val_loss']))\n",
    "print(\"Test Accuracy: {}\".format(acc))\n",
    "print(\"Max train Acc\", np.max(history_avg.history['acc']))\n",
    "\n",
    "# Save model\n",
    "RNN_max.save_model(\"model_avg.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
